{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipankerz/RVC_TPU/blob/main/RVC_TPU_0.0.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Google Colab banned the Gradio UI for free users, but you can still train models and infer locally or using a hugginface space."
      ],
      "metadata": {
        "id": "gfuLFhwnr0Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and Setup Project on Google Drive\n",
        "# This Notebook is based on another found in: https://github.com/ardha27/AI-Song-Cover-RVC\n",
        "# Visit the repo to read more and support.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ensure Google Drive is properly mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Your drive is not mounted. Creating a fake drive for demonstration purposes.\")\n",
        "    os.makedirs('/content/drive/MyDrive')\n",
        "\n",
        "# Set up the source repository\n",
        "source = \"Rejekts\"  # Replace with the desired source if different\n",
        "\n",
        "# Download and unzip project files into Google Drive\n",
        "print(\"Downloading and setting up project files...\")\n",
        "!wget https://huggingface.co/Rejekts/project/resolve/main/project-main.zip -O '/content/project-main.zip'\n",
        "!unzip -n 'project-main.zip' -d /content/drive/MyDrive\n",
        "\n",
        "# Navigate to the project directory and set up dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "!cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements-safe.txt'\n",
        "\n",
        "# Install additional dependencies\n",
        "!pip install pyngrok\n",
        "\n",
        "# Cleanup unnecessary files and create necessary directories\n",
        "print(\"Cleaning up unnecessary files...\")\n",
        "!rm /content/project-main.zip\n",
        "!rm -r /content/sample_data\n",
        "!mkdir -p /content/dataset\n",
        "\n",
        "# Clear the output to reduce clutter\n",
        "clear_output()\n",
        "\n",
        "print(\"Setup complete. Project files are now ready in Google Drive.\")\n"
      ],
      "metadata": {
        "id": "8r2dQ-aEzWLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install to Google Drive (for Resuming Training & Automatic Saving)\n",
        "#@markdown <small> This Notebook is based on another found in: https://github.com/ardha27/AI-Song-Cover-RVC < Visit this repo to read more and support.\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import clear_output\n",
        "from ipywidgets import Button\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Your drive is not mounted. Creating Fake Drive.\")\n",
        "    os.makedirs('/content/drive/MyDrive')\n",
        "source = \"Rejekts\"\n",
        "!wget https://huggingface.co/{source}/project/resolve/main/project-main.zip -O '/content/project-main.zip' && unzip -n 'project-main.zip' -d /content/drive/MyDrive\n",
        "!cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements-safe.txt'\n",
        "!pip install pyngrok\n",
        "!rm /content/project-main.zip\n",
        "!rm -r /content/sample_data\n",
        "!mkdir -p /content/dataset\n"
      ],
      "metadata": {
        "id": "Sb5fzhzEXK8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.Preprocess Data\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "model_name = 'NeilModel' #@param {type:\"string\"}\n",
        "#@markdown <small> Enter the path to your dataset folder (a folder with audios of the vocals you will train on), or if you want just upload the audios using the File Manager into the 'dataset' folder.\n",
        "dataset_folder = '/content/dataset' #@param {type:\"string\"}\n",
        "while len(os.listdir(dataset_folder)) < 1:\n",
        "    input(\"Your dataset folder is empty.\")\n",
        "!mkdir -p ./logs/{model_name}\n",
        "with open(f'./logs/{model_name}/preprocess.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "!python infer/modules/train/preprocess.py {dataset_folder} 40000 2 ./logs/{model_name} False 3.0 > /dev/null 2>&1\n",
        "with open(f'./logs/{model_name}/preprocess.log','r') as f:\n",
        "    if 'end preprocess' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your dataset folder is correct.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w4wXvoez9Rce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "AqxL70stdZND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.Extract Features\n",
        "f0method = \"rmvpe_gpu\" # @param [\"pm\", \"harvest\", \"rmvpe\", \"rmvpe_gpu\"]\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','w') as f:\n",
        "    print(\"Starting...\")\n",
        "if f0method != \"rmvpe_gpu\":\n",
        "    !python infer/modules/train/extract/extract_f0_print.py ./logs/{model_name} 2 {f0method}\n",
        "else:\n",
        "    !python infer/modules/train/extract/extract_f0_rmvpe.py 1 0 0 ./logs/{model_name} True\n",
        "!python infer/modules/train/extract_feature_print.py cuda:0 1 0 0 ./logs/{model_name} v2\n",
        "with open(f'./logs/{model_name}/extract_f0_feature.log','r') as f:\n",
        "    if 'all-feature-done' in f.read():\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))\n",
        "    else:\n",
        "        print(\"Error preprocessing data... Make sure your data was preprocessed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G0MEhFM19Vq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.Train Index\n",
        "import numpy as np\n",
        "import faiss\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "def train_index(exp_dir1, version19):\n",
        "    exp_dir = \"logs/%s\" % (exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if not os.path.exists(feature_dir):\n",
        "        return \"请先进行特征提取!\"\n",
        "    listdir_res = list(os.listdir(feature_dir))\n",
        "    if len(listdir_res) == 0:\n",
        "        return \"请先进行特征提取！\"\n",
        "    infos = []\n",
        "    npys = []\n",
        "    for name in sorted(listdir_res):\n",
        "        phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
        "        npys.append(phone)\n",
        "    big_npy = np.concatenate(npys, 0)\n",
        "    big_npy_idx = np.arange(big_npy.shape[0])\n",
        "    np.random.shuffle(big_npy_idx)\n",
        "    big_npy = big_npy[big_npy_idx]\n",
        "    if big_npy.shape[0] > 2e5:\n",
        "        infos.append(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
        "        yield \"\\n\".join(infos)\n",
        "        try:\n",
        "            big_npy = (\n",
        "                MiniBatchKMeans(\n",
        "                    n_clusters=10000,\n",
        "                    verbose=True,\n",
        "                    batch_size=256 * config.n_cpu,\n",
        "                    compute_labels=False,\n",
        "                    init=\"random\",\n",
        "                )\n",
        "                .fit(big_npy)\n",
        "                .cluster_centers_\n",
        "            )\n",
        "        except:\n",
        "            info = traceback.format_exc()\n",
        "            logger.info(info)\n",
        "            infos.append(info)\n",
        "            yield \"\\n\".join(infos)\n",
        "\n",
        "    np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
        "    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
        "    infos.append(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
        "    yield \"\\n\".join(infos)\n",
        "    index = faiss.index_factory(256 if version19 == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
        "    infos.append(\"training\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    index_ivf = faiss.extract_index_ivf(index)  #\n",
        "    index_ivf.nprobe = 1\n",
        "    index.train(big_npy)\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "\n",
        "    infos.append(\"adding\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    batch_size_add = 8192\n",
        "    for i in range(0, big_npy.shape[0], batch_size_add):\n",
        "        index.add(big_npy[i : i + batch_size_add])\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "    infos.append(\n",
        "        \"成功构建索引，added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (n_ivf, index_ivf.nprobe, exp_dir1, version19)\n",
        "    )\n",
        "\n",
        "training_log = train_index(model_name, 'v2')\n",
        "\n",
        "for line in training_log:\n",
        "    print(line)\n",
        "    if 'adding' in line:\n",
        "        clear_output()\n",
        "        display(Button(description=\"\\u2714 Success\", button_style=\"success\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3KyMRbK49g__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model Script\n",
        "# Import necessary modules\n",
        "from random import shuffle\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Global Variables\n",
        "now_dir = os.getcwd()\n",
        "\n",
        "# User Inputs\n",
        "ngrok_authtoken = \"2Z6r4I6fad3L2Chu7hSb5f9qdcm_4wTKeMnUKrguTkNg48TD3\"  # NGROK authtoken\n",
        "model_name = 'NeilModel'  # Model name\n",
        "save_frequency = 50  # Frequency to save the model\n",
        "epochs = 250  # Number of training epochs\n",
        "cache = False  # Whether to cache datasets\n",
        "\n",
        "# Configure ngrok\n",
        "os.system(f\"ngrok config add-authtoken {ngrok_authtoken}\")\n",
        "\n",
        "def setup_directories(exp_dir, gt_wavs_dir, feature_dir, f0_dir=None, f0nsf_dir=None):\n",
        "    \"\"\"Ensure necessary directories exist.\"\"\"\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    os.makedirs(gt_wavs_dir, exist_ok=True)\n",
        "    os.makedirs(feature_dir, exist_ok=True)\n",
        "    if f0_dir:\n",
        "        os.makedirs(f0_dir, exist_ok=True)\n",
        "    if f0nsf_dir:\n",
        "        os.makedirs(f0nsf_dir, exist_ok=True)\n",
        "\n",
        "def generate_filelist(gt_wavs_dir, feature_dir, spk_id, f0_dir=None, f0nsf_dir=None, if_f0=False, version=\"v2\"):\n",
        "    \"\"\"Generate the filelist for training.\"\"\"\n",
        "    names = set(name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)) & \\\n",
        "            set(name.split(\".\")[0] for name in os.listdir(feature_dir))\n",
        "\n",
        "    if if_f0:\n",
        "        names &= set(name.split(\".\")[0] for name in os.listdir(f0_dir))\n",
        "        names &= set(name.split(\".\")[0] for name in os.listdir(f0nsf_dir))\n",
        "\n",
        "    fea_dim = 256 if version == \"v1\" else 768\n",
        "    filelist = []\n",
        "\n",
        "    for name in names:\n",
        "        if if_f0:\n",
        "            filelist.append(\n",
        "                f\"{gt_wavs_dir}/{name}.wav|{feature_dir}/{name}.npy|{f0_dir}/{name}.wav.npy|\"\n",
        "                f\"{f0nsf_dir}/{name}.wav.npy|{spk_id}\"\n",
        "            )\n",
        "        else:\n",
        "            filelist.append(f\"{gt_wavs_dir}/{name}.wav|{feature_dir}/{name}.npy|{spk_id}\")\n",
        "\n",
        "    # Adding muted entries\n",
        "    for _ in range(2):\n",
        "        if if_f0:\n",
        "            filelist.append(\n",
        "                f\"{now_dir}/logs/mute/0_gt_wavs/mute40k.wav|{now_dir}/logs/mute/3_feature{fea_dim}/mute.npy|\"\n",
        "                f\"{now_dir}/logs/mute/2a_f0/mute.wav.npy|{now_dir}/logs/mute/2b-f0nsf/mute.wav.npy|{spk_id}\"\n",
        "            )\n",
        "        else:\n",
        "            filelist.append(\n",
        "                f\"{now_dir}/logs/mute/0_gt_wavs/mute40k.wav|{now_dir}/logs/mute/3_feature{fea_dim}/mute.npy|{spk_id}\"\n",
        "            )\n",
        "\n",
        "    shuffle(filelist)\n",
        "    return filelist\n",
        "\n",
        "def click_train(exp_dir, model_config, filelist):\n",
        "    \"\"\"Execute the training process.\"\"\"\n",
        "    # Save the filelist\n",
        "    filelist_path = os.path.join(exp_dir, \"filelist.txt\")\n",
        "    with open(filelist_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(filelist))\n",
        "\n",
        "    print(\"Filelist written successfully.\")\n",
        "\n",
        "    # Training command\n",
        "    cmd = (\n",
        "        f'python infer/modules/train/train.py -e \"{model_config[\"exp_dir\"]}\" -sr {model_config[\"sample_rate\"]} '\n",
        "        f'-f0 {1 if model_config[\"if_f0\"] else 0} -bs {model_config[\"batch_size\"]} -g {model_config[\"gpus\"]} '\n",
        "        f'-te {model_config[\"total_epochs\"]} -se {model_config[\"save_epochs\"]} '\n",
        "        f'{f\"-pg {model_config[\"pretrained_G\"]}\" if model_config[\"pretrained_G\"] else \"\"} '\n",
        "        f'{f\"-pd {model_config[\"pretrained_D\"]}\" if model_config[\"pretrained_D\"] else \"\"} '\n",
        "        f'-l {1 if model_config[\"save_latest\"] else 0} -c {1 if model_config[\"cache_gpu\"] else 0} '\n",
        "        f'-sw {1 if model_config[\"save_weights\"] else 0} -v {model_config[\"version\"]}'\n",
        "    )\n",
        "\n",
        "    # Run the command\n",
        "    with Popen(cmd, shell=True, cwd=now_dir, stdout=PIPE, stderr=STDOUT, bufsize=1, universal_newlines=True) as proc:\n",
        "        for line in proc.stdout:\n",
        "            print(line.strip())\n",
        "        proc.wait()\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "    return \"Training finished. Check logs for details.\"\n",
        "\n",
        "# Tensorboard setup\n",
        "from IPython import get_ipython\n",
        "get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
        "get_ipython().run_line_magic(\"tensorboard\", \"--logdir ./logs --port=8888\")\n",
        "\n",
        "# Start ngrok for tensorboard\n",
        "print(\"Tensorboard NGROK URL:\", end=\"\")\n",
        "try:\n",
        "    ngrok_url = ngrok.connect(8888)\n",
        "    print(ngrok_url)\n",
        "    model_config = {\n",
        "        \"exp_dir\": model_name,\n",
        "        \"sample_rate\": \"40k\",\n",
        "        \"if_f0\": True,\n",
        "        \"batch_size\": 7,\n",
        "        \"save_epochs\": save_frequency,\n",
        "        \"total_epochs\": epochs,\n",
        "        \"pretrained_G\": \"assets/pretrained_v2/f0G40k.pth\",\n",
        "        \"pretrained_D\": \"assets/pretrained_v2/f0D40k.pth\",\n",
        "        \"gpus\": 0,\n",
        "        \"cache_gpu\": cache,\n",
        "        \"save_latest\": True,\n",
        "        \"save_weights\": True,\n",
        "        \"version\": \"v2\"\n",
        "    }\n",
        "    gt_wavs_dir = f\"{now_dir}/logs/{model_name}/0_gt_wavs\"\n",
        "    feature_dir = f\"{now_dir}/logs/{model_name}/3_feature768\"\n",
        "    setup_directories(model_name, gt_wavs_dir, feature_dir)\n",
        "    filelist = generate_filelist(gt_wavs_dir, feature_dir, 0, if_f0=True)\n",
        "    training_log = click_train(model_name, model_config, filelist)\n",
        "    print(training_log)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "    ngrok.kill()\n"
      ],
      "metadata": {
        "id": "FFfC9x239kC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OR Open the GUI (Banned for free Colab Notebooks)\n",
        "if not 'installed' in locals():\n",
        "    %cd /content\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from IPython.display import clear_output\n",
        "    from ipywidgets import Button\n",
        "    import os\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Your drive is not mounted. Creating Fake Drive.\")\n",
        "        os.makedirs('/content/drive/MyDrive')\n",
        "    if not os.path.exists('/content/drive/MyDrive/project-main'):\n",
        "        !wget https://huggingface.co/Rejekts/project/resolve/main/project-main.zip -O '/content/project-main.zip' && unzip 'project-main.zip' -d /content/drive/MyDrive\n",
        "    !cd '/content/drive/MyDrive/project-main' && python download_files.py && pip install -r 'requirements.txt'\n",
        "    !rm /content/project-main.zip\n",
        "    !rm -r /content/sample_data\n",
        "    !mkdir -p /content/dataset\n",
        "    clear_output()\n",
        "    Button(description=\"\\u2714 Success\", button_style=\"success\")\n",
        "tensorboard = True #@param {type:\"boolean\"}\n",
        "if tensorboard:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir ./logs\n",
        "%cd /content/drive/MyDrive/project-main\n",
        "!python app.py --colab"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DZDKirCM0F9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}